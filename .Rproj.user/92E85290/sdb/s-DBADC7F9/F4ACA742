{
    "contents" : "#' Converts bibliographic text files extracted from EbscoHost into analyzable data frame\n#' @param path path directory for the folder containing text files to be wrangled\n#' @param rmDuplicates logical vector indicating whether the function should identify and remove duplicate article records\n#' @param firstAbstractOnly locgical vector indicating whether to retain all available abstract fields for a given article record.  See front end notation. \n#' @param csv logical vector indicating whether the final data frame generated by the function should be saved to the working directory as a .csv file\n#' @examples ebscoBWR.f(csv=TRUE, path=\"C:/Users/JaneDoe/Desktop/EbscoHostTextFiles\")\n#' @note ebsoBWR.f will wrangle all text files present in the path folder.  \n#' @export\n\n\nebscoBWR.f <- function(path, rmDuplicates = TRUE, firstAbstractOnly=TRUE, csv=FALSE){\n  \n  #_______________________________________________________________________________\n  #                           MAIN TO-DO LIST\n  #-------------------------------------------------------------------------------\n  #\n  #\n  #  4. Include note on importance of naming files to give priority\n  #\n  #\n  #\n  #  8.  Include a quality check at end of later sections to ensure TI and SO\n  #      match and are not reduced in subsequent parsing.\n  #_______________________________________________________________________________\n  \n  \n  #_______________________________________________________________________________\n  #                       0. Install Missing Packages\n  #-------------------------------------------------------------------------------\n  #\n  #All files to be wrangled should be saved in a single folder and have a *.txt\n  #extension.  The files must be processed from EbscoHost in the generic\n  #bibliographic format -- no other file structure will work.\n  #\n  #_______________________________________________________________________________\n  \n  \n  pkgs <- c(\"dplyr\", \"stringi\", \"stringr\")\n  pkgs_miss <- pkgs[which(!pkgs %in% installed.packages()[, 1])]\n  if (length(pkgs_miss) > 0) {\n    message(\"\\n ...Installing missing packages!\\n\")\n    install.packages(pkgs_miss)\n  }\n  \n  if (length(pkgs_miss) == 0) {\n    message(\"\\n ...Packages were already installed!\\n\")\n  }\n  \n  \n  #_______________________________________________________________________________\n  #                           1a. READ EBSCO txt files\n  #-------------------------------------------------------------------------------\n  #\n  #All files to be wrangled should be saved in a single folder and have a *.txt\n  #extension.  The files must be processed from EbscoHost in the generic\n  #bibliographic format -- no other file structure will work.\n  #\n  #_______________________________________________________________________________\n  \n  library(dplyr)\n  temp <- list.files(path, pattern = \".txt\", full.names=TRUE)\n  \n  readUTF <- function(x){\n    x <- readLines(x, encoding = \"utf-8\")\n  }\n  \n  dat <- lapply(temp, readUTF)\n  \n  attributes <- unlist(lapply(dat, function(x) stringi::stri_sub(x, 1,2)))\n  \n  attributes.df <- data.frame(attributes)\n  \n  #Extracts the character string in \"dat\" beginning with the fifth character(attribute codes and\n  #punctuation are contained in characters 1-4). These character strings are the record values.\n  \n  record <- substring(unlist(dat), 5)\n  \n  record.df <- data.frame(record)\n  \n  DF <- cbind(attributes.df, record.df)\n  \n  DF <- data.frame(lapply(DF, as.character), stringsAsFactors = FALSE)\n  \n  rm(temp, dat, attributes, attributes.df, record, record.df)\n  \n  #_______________________________________________________________________________\n  #                           1b. Create article IDs\n  #-------------------------------------------------------------------------------\n  #\n  #Each article record contains a set of attributes related to the scholarly article including\n  #title, journal, author, author affilitation, year published,location in which the research \n  #was conducted, page length, etc. To be able to retrieve all available data for a given article\n  # record, the same articleID is assigned to all attributes of that record. \n  #_______________________________________________________________________________\n  \n  #Most article records are divided by a blank row, however this is not always the case. The\n  #following code assesses for a blank row following an article record which always ends with \n  #the UR attribute field.  If a blank row is not present following an article record, one will\n  #be added.\n  \n  blank <- c(\"\", \"\", \"\")\n  DF <- rbind(blank, DF)\n  \n  DF.temp <- DF\n  \n  DF.temp$tempID <- as.numeric(1:length(DF.temp$attributes))\n  \n  \n  x1 <- which(DF.temp$attributes == \"UR\")\n  x2 <- x1+1\n  DF.x2 <- DF.temp[x2, ]\n  duplicate <- filter(DF.x2, attributes != \"\")\n  \n  ID <- as.numeric(duplicate$tempID)\n  ID.placement <- as.numeric(ID - 0.5)\n  \n  blank.rows <- rep(\"\", length(ID.placement))\n  \n  blank.frame <- cbind(blank.rows, blank.rows)\n  blank.frame <- as.data.frame(cbind(blank.frame, as.numeric(ID.placement)))\n  \n  colnames(blank.frame) <- c(\"attributes\", \"record\", \"tempID\")\n  blank.frame$tempID <- as.character(blank.frame$tempID)\n  blank.frame$tempID <- as.numeric(blank.frame$tempID)\n  \n  DF.temp <- rbind(DF.temp, blank.frame)\n  \n  DF.temp <- DF.temp[order(DF.temp$tempID),]\n  \n  DF <- select(DF.temp, -tempID)\n  rm(DF.temp, blank.frame, blank.rows, ID, ID.placement, x1, x2, DF.x2, duplicate)\n  \n  ##articleIDs assigned following insertion of blank row following UR entries\n  DF$articleID <- cumsum(DF$attributes ==\"\")\n  \n    \n  #_______________________________________________________________________________\n  #                     2a. LABEL SECOND TI FIELDS\n  #-------------------------------------------------------------------------------\n  #\n  # Some records contain multiple TI fields when there is a translated title or the article \n  # titlewas unintentially split between two fields. To get an accurate count of the number \n  # of unique titles, the extra TI fields must be re-labeled.  \n  #______________________________________________________________________________\n\n  \n  DF<-DF[order(DF$articleID, DF$attributes),]\n  \n  tempID <- 1:length(DF$record)\n  DF.temp <- cbind(DF, tempID)\n  \n  x1 <- which(DF.temp$attributes == \"TI\")\n  x2 <- x1+1\n  DF.x2 <- DF.temp[x2, ]\n  duplicate <- filter(DF.x2, attributes == \"TI\")\n  duplicate.ind <- duplicate$tempID\n  \n  DF.holder <- mutate(DF.temp, attributes=ifelse(tempID%in%duplicate.ind, \"secondTitle\", attributes))\n  \n  DF <- select(DF.holder, -tempID)\n  \n  rm(DF.temp, DF.holder, DF.x2, duplicate, duplicate.ind, tempID, x1, x2)\n  \n  #_______________________________________________________________________________\n  #                     2b. REMOVE MULTIPLE AB FIELDS\n  #-------------------------------------------------------------------------------\n  #\n  # SSA records contain two AB fields.  These AB records are always serially\n  # serially ordered, so the extraction method in 2a is reapplied here to detect the presence of\n  #more than one AB attribute for a given article record.  An if statement is built in to allow\n  #the user to retain mutliple AB fields for article records if they desire, but the default \n  #setting is that those abstract fields beyond the first be removed.\n  #_______________________________________________________________________________\n  \n\n  if(firstAbstractOnly==TRUE)\n    \n {tempID <- 1:length(DF$record)\n  DF.temp <- cbind(DF, tempID)\n  \n  x1 <- which(DF.temp$attributes == \"AB\")\n  x2 <- x1+1\n  DF.x2 <- DF.temp[x2, ]\n  duplicate <- filter(DF.x2, attributes == \"AB\")\n  duplicate.ind <- duplicate$tempID\n  \n  DF.holder <- DF.temp[!(DF.temp$tempID %in% duplicate.ind),]\n  \n  DF <- select(DF.holder, -tempID)\n  \n  rm(DF.temp, DF.holder, DF.x2, duplicate, duplicate.ind, tempID, x1, x2)\n  }\n  \n    \n  #_______________________________________________________________________________\n  #                    3.  REMOVE DUPLICATE RECORDS\n  #-------------------------------------------------------------------------------\n  # In the prior section, second title (TI) fields were identified and re-labeled.\n  # These were non-matching titles because, for a given article, either one title is in\n  # English and the other is in a foreing language, or the subtitle was placed in a second TI  \n  # field. In this section, the code is doing a global match for duplicate article records \n  # based on the title. Duplicates occur because multiple databases have overlapping indexing.\n  #_______________________________________________________________________________\n  \n  if(rmDuplicates==TRUE)\n    \n  #Select out all titles\n  {DF.temp <- filter(DF, attributes == \"TI\")\n  \n  #Journal titles show discrepancies in capitalization rules.  Force all to\n  #lower to address this problem.  Further testing should consider stripping\n  #white space.\n  DF.temp$record <- tolower(DF.temp$record)\n  \n  #Find duplicated records - duplicates are marked as true\n  DF.temp <- DF.temp[duplicated(DF.temp$record), ]\n  \n  #Screen out duplicated records by articleID. The articleID must be used\n  #because the duplicate title contains other article attributes\n  DF.duplicated.ID <- DF.temp$articleID\n  DF <- DF[!(DF$articleID %in% DF.duplicated.ID), ]\n  \n  rm(DF.temp, DF.duplicated.ID)\n  }\n  \n  #_______________________________________________________________________________\n  #     4.  CLEAN JOURNAL NAMES AND MERGE JOURNAL NAME FIELDS (SO AND JN)\n  #-------------------------------------------------------------------------------\n  #\n  #Journals that have a Special Issue are not grouped together as the same\n  #title. These subtitles need to be removed.  Some articles have both the JN\n  #and SO code.  Thus, keeping both results in an excess count. A quality control\n  #check is to ensure the number of article titles is exactly equal to the\n  #unique number of journal title entries\n  #_______________________________________________________________________________\n  \n  \n  DF$record[DF$attributes == \"SO\"] <- gsub(\" Special Issue\", \"\",\n                                           DF$record[DF$attributes == \"SO\"])\n  \n  DF$record[DF$attributes == \"SO\"] <- gsub(\":.*\", \"\",\n                                           DF$record[DF$attributes == \"SO\"])\n  \n  DF$record[DF$attributes == \"JN\"] <- gsub(\" Special Issue\", \"\",\n                                           DF$record[DF$attributes == \"JN\"])\n  \n  DF$record[DF$attributes == \"JN\"] <- gsub(\":.*\", \"\",\n                                           DF$record[DF$attributes == \"JN\"])\n  \n  #Create separate data files filtered by SO and JN, and a set of unique ID's\n  journal.unique.SO <- filter(DF, attributes == \"SO\")\n  journal.unique.JN <- filter(DF, attributes == \"JN\")\n  articleID.unique <- unique(DF$articleID)\n  \n  #Which ID's overlap from JN to SO? This shows articles with both JN and SO\n  #fields.\n  JN.in.SO <- journal.unique.JN$articleID %in% journal.unique.SO$articleID\n  \n  #Filter out the ID from the JN that overlap with SO\n  journal.unique.JN <- journal.unique.JN[!(JN.in.SO), ]\n  journal.unique.JN <- mutate(journal.unique.JN, attributes = \"SO\")\n  DF <- filter(DF, attributes != \"JN\")\n  DF <- rbind(DF, journal.unique.JN)\n  \n  rm(journal.unique.SO, journal.unique.JN, articleID.unique, JN.in.SO)\n  \n  #_______________________________________________________________________________\n  #                        5. COMBINE YEAR FIELDS\n  #-------------------------------------------------------------------------------\n  #\n  # Year fields must be combined because each database uses a separate code.\n  # psychInfo uses the YR field; SSA, PD; and SWA, PD.  It is not possible to\n  # Simply rename PD to YR, because psycInfo also uses a PD field as another\n  # variable.  Thus, the PD values that are specific to psycInfo need to be\n  # eliminated before it can be replaced by the year (PD) field from SSA.\n  #_______________________________________________________________________________\n  \n  \n  \n  \n  DF$attributes <- ifelse(DF$attributes == \"PY\", \"YR\", DF$attributes)\n  \n  DF.temp <- DF\n  DF.temp <- filter(DF.temp, attributes == \"PD\")\n  \n  \n  # PsycINFO ocassionally marks an articles PubMed ID using the \"PD\" code.  These are \n  #6-8 digits long, and so a regular expression is used to detect them and rename the attribute\n  DF.temp<- DF.temp %>%\n    mutate(attributes=ifelse(grepl(\"[0-9]{6,8}\", record), \"pubMedID\", attributes))\n  \n  #Eliminates \"Bibiliography\", \"Graph\" and \"Table\" from PD field\n  DF.temp$record<-sub(\"[BGT]?[a-z]{4,11}\", \"\", DF.temp$record)\n  \n  # Extract the first portion of the dates, up to the point with a 2 or 4\n  # digit year value. This also captures some letters and characters.\n  DF.temp$record <- stringr::str_extract(DF.temp$record,\n                                         \"[$/A-Za-z0-9]+\\\\d{2,4}\")\n  \n  #Exclude the characters\n  DF.temp$record <- gsub(\"[/A-Za-z]\", \"\", DF.temp$record)\n  \n  #Add 19 to all the records with just two digits\n  DF.temp <- DF.temp[!is.na(DF.temp$record), ]\n  DF.flag <- mutate(DF.temp,\n                    flag = ifelse(nchar(DF.temp$record) == 2, \"1\", \"0\" )) %>%\n    filter(flag == \"1\") %>%\n    group_by(record) %>%\n    summarize(N = n())\n  \n  colnames(DF.flag)<-c(\"Year\", \"N\")\n  \n  DF.temp$record <- ifelse(as.numeric(DF.temp$record) < 100,\n                           paste(\"19\", DF.temp$record, sep=\"\"), DF.temp$record)\n  DF.temp <- mutate(DF.temp, attributes = ifelse(attributes==\"pubMedID\", \"pubMedID\", \"YR\"))\n  \n  DF <- rbind(DF, DF.temp)\n  DF <- arrange(DF, articleID)\n  \n  rm(DF.temp)\n  \n  \n  #_______________________________________________________________________________\n  #                            6a. AUTHOR FIELD FIX - DIGITS\n  #-------------------------------------------------------------------------------\n  #\n  # For articles pulled in from SSA, superscripts used to footnote affiliation get\n  # added to author's first name in the AU field\n  #_______________________________________________________________________________\n  \n  DF.temp <- filter(DF, attributes == \"AU\")\n  DF.temp$record <- gsub(\"[[:digit:]]\", \"\", DF.temp$record)\n  DF <- filter(DF, attributes != \"AU\")\n  \n  DF <- rbind(DF, DF.temp)\n  DF <- arrange(DF, articleID)\n  rm(DF.temp)\n  \n  #_______________________________________________________________________________\n  #            6b. AUTHOR FIELD FIX - AUTHORS IN SINGLE FIELD\n  #-------------------------------------------------------------------------------\n  #\n  # Social Work abstracts lists authors in a single cell, separated by a semi-\n  # colon, and then includes digits and email addresses in some occassions.\n  # The following text locates each author in the cell and places it into a new\n  # row to be consistent with PsychInfo and SSA.\n  #_______________________________________________________________________________\n  \n  \n  #Create a new temporary data frame that will be used to check for the presence \n  #of semi-colons in the author field\n  DF.temp <- filter(DF, attributes == \"AU\")\n  \n  #Identify records with semi-colons in author names\n  semi.colons <- grepl(\"(;)\", DF.temp$record)\n  \n  #Select out those records with semi-colons in author names from temporary\n  #data frame\n  DF.temp <- DF.temp[semi.colons, ]\n  \n  #The following will be run if semi-colons are present.\n  #First, add a semi colon to the end of every string\n  if(length(DF.temp$record>=1)){DF.temp$record <- paste(DF.temp$record, \";\", sep=\"\")\n  semi.colon.split <- strsplit(DF.temp$record, \";\")\n  \n  split.df <- data.frame(\n    attributes = rep(DF.temp$attributes, lapply(semi.colon.split, length)),\n    record = unlist(semi.colon.split),\n    articleID = rep(DF.temp$articleID, lapply(semi.colon.split, length)))\n  \n  #Trim whitespace on both sides\n  split.df$record <- stringr::str_trim(split.df$record, side = \"both\")\n  \n  \n  #The author field is problematic because it contains some email addresses.\n  #Some fields have been improperly split because they were split on a semi-colon\n  #that was in the middle of the filed.\n  \n  #Create a pattern that eliminates possible emails\n  split.df$record <- ifelse(grepl(\"@\", split.df$record) == TRUE, \"\",\n                            split.df$record)\n  split.df$record <- ifelse(nchar(split.df$record) <= 3, \"\",\n                            split.df$record)\n  split.df$record <- ifelse(grepl(\"\\\\.\", split.df$record) == FALSE, \"\",\n                            split.df$record)\n  split.df$record <- ifelse(grepl(\"&\", split.df$record) == TRUE, \"\",\n                            split.df$record)\n  split.df <- filter(split.df, record != \"\")\n  \n  # Create a vector of all articleID's that were fixed\n  fixed.ID <- unique(split.df$articleID)\n  \n  #Filter out all processed records from the fixed list\n  DF.authors <- filter(DF, attributes == \"AU\")\n  DF.authors.good <- DF.authors[!(DF.authors$articleID %in% fixed.ID),]\n  DF.authors.fixed <- split.df\n  DF.no.authors <- filter(DF, attributes != \"AU\")\n  \n  #Bind the reduced DF with the fixed df\n  DF <- rbind(DF.no.authors, DF.authors.good, DF.authors.fixed)\n  DF <- arrange(DF, articleID)\n  }\n  \n  rm(DF.temp)\n  \n  #_______________________________________________________________________________\n  #                       6c. E-mails\n  #-------------------------------------------------------------------------------\n  #\n  # After fixing the author fields in 6b, subsequent testing revealed a large\n  # number of email addresses that were still in the datafile and note excluded.\n  # This section is a patch for this issue.  This code should be re-written\n  # and integrated with the prior section.\n  #_______________________________________________________________________________\n  \n  DF.temp <- filter(DF, attributes == \"AU\")\n  sub.1 <- sub(\"^([^,]*,[^,]*),.*\", \"\\\\1\", DF.temp$record)\n  sub.2 <- sub(\"[,\\\\.][a-zA-Z]{1,}@\", \"\", sub.1)\n  sub.3 <- sub(\"@[a-zA-Z0-9.\\\\]{1,}\", \"\", sub.2)\n  sub.4 <- sub(\"(\\\\s[a-z]{1,})$\", \"\", sub.3)\n  sub.5 <- sub(\"(/&;#]+)\", \"\", sub.4)\n  DF.temp$record <- sub.5\n  \n  DF <- filter(DF, attributes != \"AU\")\n  DF <- rbind(DF, DF.temp)\n  DF <- arrange(DF, articleID)\n  rm(DF.temp)\n  \n  #_______________________________________________________________________________\n  #                       7. Minor Cleaning\n  #-------------------------------------------------------------------------------\n  #\n  # In this section, meaningful variable names are assigned to variables that have\n  # been cleaned and are appropriate for analysis.  All other variables are\n  # excluded to prevent inappropriate analyses.\n  #_______________________________________________________________________________\n  \n  # Exclude UR record from the data file\n  DF$attributes <- ifelse(DF$attributes == \"KW\", \"KP\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"AD\", \"AF\", DF$attributes)\n  \n  DF$attributes <- ifelse(DF$attributes == \"TI\", \"article\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"AU\", \"author\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"SO\", \"journal\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"YR\", \"pubYear\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"AB\", \"abstract\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"KP\", \"keyWord\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"LO\", \"location\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"S2\", \"journalSecondary\",\n                          DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"AF\", \"authorAff\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"PG\", \"pages\", DF$attributes)\n  \n  variables.to.keep <- c(\"article\", \"author\", \"journal\", \"pubYear\", \"abstract\",\n                         \"keyWord\", \"location\", \"journalSecondary\", \"authorAff\", \n                         \"pages\", \"pubMedID\", \"secondTitle\")\n  \n  DF <- DF[DF$attributes %in% variables.to.keep, ]\n  \n  # Strip white-space\n  DF$record <- stringr::str_trim(DF$record, side=\"both\")\n  \n  DF$record <- ifelse(DF$attributes == \"keyWord\", tolower(DF$record), DF$record)\n  \n  \n  # Remove rownames\n  rownames(DF) <- NULL\n  \n  # Reorder variables\n  DF <- select(DF, articleID, attributes, record)\n  \n  rm(blank, path, sub.1, sub.2, sub.3, sub.4, sub.5, variables.to.keep)\n  #_______________________________________________________________________________\n  #                        8. OUTPUT\n  #-------------------------------------------------------------------------------\n  #\n  # This final section places a datafile in the global environment, which is\n  # called ebscoBWR.df.  If csv is specified as TRUE in the ebscoBWR function call,\n  # a csv file is written to the user's current working directory.  A few messages\n  # are written to the user's screen, providing a warning message and a few\n  # quality checks to ensure the number of articles matches the number of sources.\n  #_______________________________________________________________________________\n  \n  ebscoBWR.df <<- DF\n  \n  rm(DF)\n  if(csv == TRUE){write.csv(ebscoBWR.df, \"ebscoBWR.csv\")}\n  \n  cat(\n    \"****************************************************\\n          Wrangling is complete\\n****************************************************\")\n  \n  if(csv == TRUE){cat(\n    \"\\nThe `ebscoBWR.csv` file can be found in your working directory:\\n\", getwd())}\n}\n\n",
    "created" : 1441033652242.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1139508394",
    "id" : "F4ACA742",
    "lastKnownWriteTime" : 1441383785,
    "path" : "~/GitHub/BibWrangleR/R/ebscoBWR.R",
    "project_path" : "R/ebscoBWR.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}