{
    "contents" : "#' Converts bibliographic text files extracted from EbscoHost into analyzable data frame\n#' @param csv - logical vector indicating whether the final data frame generated by the function should be saved to the working directory as a .csv file\n#' @param path - path directory for the folder containing text files to be wrangled\n#' @examples ebscoBWR.f(csv=TRUE, path=\"C:/Users/JaneDoe/Desktop/EbscoHostTextFiles\")\n#' @note ebsoBWR.f will wrangle all text files present in the path folder.  \n#' @export\n\n\nebscoBWR.f <- function(csv = FALSE, path){\n  \n  #_______________________________________________________________________________\n  #                           MAIN TO-DO LIST\n  #-------------------------------------------------------------------------------\n  #\n  #  1. Write code to ensure user has required packages dplyr, stringi, stringr\n  #     and there are no conflicts with the code\n  #\n  #  4. Include note on importance of naming files to give priority\n  #\n  #  5. Integrate into Section 1 a function that checks each file for a blank line\n  #     at the end each text file.  Right now, a full carriage return must be\n  #     included, and no further quality checks are made.\n  #\n  #  6. Double check logic of section 4. Can we confirm that JN and SO are\n  #     problematic?\n  #\n  #  7. In Section 5, perform a test to ensure all APA codes are digits before\n  #     removing.\n  #\n  #  8.  Include a quality check at end of later sections to ensure TI and SO\n  #      match and are not reduced in subsequent parsing.\n  #_______________________________________________________________________________\n  \n  \n  #_______________________________________________________________________________\n  #                       0. Install Missing Packages\n  #-------------------------------------------------------------------------------\n  #\n  #All files to be wrangled should be saved in a single folder and have a *.txt\n  #extension.  The files must be processed from EbscoHost in the generic\n  #bibliographic format -- no other file structure will work.\n  #\n  #_______________________________________________________________________________\n  \n  \n  # leave this path for testing...\n  #path <- \"/Users/beperron/Git/SocialWorkResearch/Data/ebscoFULL\"\n  \n  pkgs <- c(\"dplyr\", \"stringi\", \"stringr\")\n  pkgs_miss <- pkgs[which(!pkgs %in% installed.packages()[, 1])]\n  if (length(pkgs_miss) > 0) {\n    message(\"\\n ...Installing missing packages!\\n\")\n    install.packages(pkgs_miss)\n  }\n  \n  if (length(pkgs_miss) == 0) {\n    message(\"\\n ...Packages were already installed!\\n\")\n  }\n  \n  \n  #_______________________________________________________________________________\n  #                           1. READ EBSCO txt files\n  #-------------------------------------------------------------------------------\n  #\n  #All files to be wrangled should be saved in a single folder and have a *.txt\n  #extension.  The files must be processed from EbscoHost in the generic\n  #bibliographic format -- no other file structure will work.\n  #\n  #_______________________________________________________________________________\n  \n  library(dplyr)\n  temp <- list.files(path, pattern = \".txt\", full.names=TRUE)\n  \n  readUTF <- function(x){\n    x <- readLines(x, encoding = \"utf-8\")\n  }\n  \n  dat <- lapply(temp, readUTF)\n  \n  attributes <- unlist(lapply(dat, function(x) stringi::stri_sub(x, 1,2)))\n  \n  attributes.df <- data.frame(attributes)\n  \n  #Take first five characters from each row. These are the names of the record\n  #values.\n  record <- substring(unlist(dat), 5)\n  \n  record.df <- data.frame(record)\n  \n  DF <- cbind(attributes.df, record.df)\n  \n  DF <- data.frame(lapply(DF, as.character), stringsAsFactors = FALSE)\n  \n  rm(temp, dat, attributes, attributes.df, record, record.df)\n  \n  # Create articleIDs\n  blank <- c(\"\", \"\", \"\")\n  DF <- rbind(blank, DF)\n  \n  \n  variables.to.keep <- c(\"KW\", \"KP\", \"AD\", \"AF\", \"TI\", \"AU\", \"SO\", \"YR\", \"AB\", \"LO\", \"S2\", \"AF\", \"PY\", \"JN\", \"PD\", \"PG\")\n  \n  DF <- DF[DF$attributes %in% variables.to.keep, ]\n  \n  \n  \n  #_______________________________________________________________________________\n  #                     2a. REMOVE MULTIPLE TI FIELDS\n  #-------------------------------------------------------------------------------\n  #\n  # Some records contain multiple TI fields when there is a translated title. To\n  # get an accurate count of the number of unique titles, the extra TI fields must\n  # be eliminated.  It is assumed that English titles are recorded first, and the\n  # second title is a foreing language.  The following code eliminates the foreign\n  # language.  This issue needs to be checked in occassions when searches are\n  # performed on the title itself.\n  #\n  #_______________________________________________________________________________\n  #Create an article identifier to group all records for a unique article.\n  \n  \n  x1 <- which(DF$attributes == \"TI\")\n  x2 <- x1+1\n  DF.x2 <- DF[x2, ]\n  duplicate <- which(DF.x2$attributes == \"TI\")\n  duplicate.ind <- as.numeric(row.names(DF.x2[duplicate, ]))\n  \n  DF <- DF[-(duplicate.ind),]\n  \n  #_______________________________________________________________________________\n  #                     2b. REMOVE MULTIPLE AB FIELDS\n  #-------------------------------------------------------------------------------\n  #\n  # SSA records contain two AB fields.  These AB records are always serially\n  # serially ordered, so the extraction method in 2a is reapplied here.\n  #_______________________________________________________________________________\n  \n  x1 <- which(DF$attributes == \"AB\")\n  x2 <- x1+1\n  DF.x2 <- DF[x2, ]\n  \n  table(DF.x2$attributes)\n  duplicate <- which(DF.x2$attributes == \"AB\")\n  duplicate.ind <- as.numeric(row.names(DF.x2[duplicate, ]))\n  \n  DF <- DF[-(duplicate.ind),]\n  \n  \n  #_______________________________________________________________________________\n  #                    3.  REMOVE DUPLICATE RECORDS\n  #-------------------------------------------------------------------------------\n  # In the prior section, duplicate title (TI) fields were identified and excluded.\n  # These were non-matching titles because, for a given article, one title is in\n  # English and the other is in a foreing language.  In this section, the code is\n  # doing a global match for duplicate article records based on the title.\n  # Duplicates occur because multiple databases have overlapping indexing.\n  #_______________________________________________________________________________\n  \n  #indx <- which(DF$attributes==\"TI\")\n  \n  \n  \n  #What attributes precede TI (QUALITY CHECK)\n  #DF.temp <- DF[indx-1, ]\n  #table(DF.temp$attributes, useNA = \"always\")\n  \n  \n  #SU precedes TI without a blank space for a large number of records;\n  #Use following code to insert a blank row immediately preceding all TI records\n  #just to be safe. It creates additional article records that are blanks that\n  #can be easily removed\n  \n  #newrow <- c(\"\", \"\")\n  #insertRow <- function(DF, newrow, indx) {\n  #    DF[seq(indx-1,] <- [seq(indx,nrow(DF)),]\n  #    DF[indx,] <- newrow\n  #    DF\n  #}\n  \n  \n  \n  #DF.temp.2 <- filter(DF.temp, attributes == \"SU\")\n  \n  #DF$attributes[indx-1] <- \"\"\n  DF$articleID <- cumsum(DF$attributes == \"TI\")\n  \n  \n  \n  \n  #Select out all titles\n  DF.temp <- filter(DF, attributes == \"TI\")\n  \n  #Journal titles show discrepancies in capitalization rules.  Force all to\n  #lower to address this problem.  Further testing should consider stripping\n  #white space.\n  DF.temp$record <- tolower(DF.temp$record)\n  \n  #Find duplicated records - duplicates are marked as true\n  DF.temp <- DF.temp[duplicated(DF.temp$record), ]\n  \n  #Screen out duplicated records by articleID. The articleID must be used\n  #because the duplicate title contains other article attributes\n  DF.duplicated.ID <- DF.temp$articleID\n  DF <- DF[!(DF$articleID %in% DF.duplicated.ID), ]\n  \n  rm(DF.temp, DF.duplicated.ID)\n  \n  \n  #_______________________________________________________________________________\n  #     4.  CLEAN JOURNAL NAMES AND MERGE JOURNAL NAME FIELDS (SO AND JN)\n  #-------------------------------------------------------------------------------\n  #\n  #Journals that have a Special Issue are not grouped together as the same\n  #title. These subtitles need to be removed.  Some articles have both the JN\n  #and SO code.  Thus, keeping both results in an excess count. A quality control\n  #check is to ensure the number of article titles is exactly equal to the\n  #unique number of journal title entries\n  #_______________________________________________________________________________\n  \n  \n  DF$record[DF$attributes == \"SO\"] <- gsub(\" Special Issue\", \"\",\n                                           DF$record[DF$attributes == \"SO\"])\n  \n  DF$record[DF$attributes == \"SO\"] <- gsub(\":.*\", \"\",\n                                           DF$record[DF$attributes == \"SO\"])\n  \n  DF$record[DF$attributes == \"JN\"] <- gsub(\" Special Issue\", \"\",\n                                           DF$record[DF$attributes == \"JN\"])\n  \n  DF$record[DF$attributes == \"JN\"] <- gsub(\":.*\", \"\",\n                                           DF$record[DF$attributes == \"JN\"])\n  \n  #Create separate data files filtered by SO and JN, and a set of unique ID's\n  journal.unique.SO <- filter(DF, attributes == \"SO\")\n  journal.unique.JN <- filter(DF, attributes == \"JN\")\n  articleID.unique <- unique(DF$articleID)\n  \n  #Which ID's overlap from JN to SO? This shows articles with both JN and SO\n  #fields.\n  JN.in.SO <- journal.unique.JN$articleID %in% journal.unique.SO$articleID\n  \n  #Filter out the ID from the JN that overlap with SO\n  journal.unique.JN <- journal.unique.JN[!(JN.in.SO), ]\n  journal.unique.JN <- mutate(journal.unique.JN, attributes = \"SO\")\n  DF <- filter(DF, attributes != \"JN\")\n  DF <- rbind(DF, journal.unique.JN)\n  \n  rm(journal.unique.SO, journal.unique.JN, articleID.unique, JN.in.SO)\n  \n  #_______________________________________________________________________________\n  #                        5. COMBINE YEAR FIELDS\n  #-------------------------------------------------------------------------------\n  #\n  # Year fields must be combined because each database uses a separate code.\n  # psychInfo uses the YR field; SSA, PD; and SWA, PD.  It is not possible to\n  # Simply rename PD to YR, because psycInfo also uses a PD field as another\n  # variable.  Thus, the PD values that are specific to psycInfo need to be\n  # eliminated before it can be replaced by the year (PD) field from SSA.\n  #_______________________________________________________________________________\n  \n  \n  \n  \n  DF$attributes <- ifelse(DF$attributes == \"PY\", \"YR\", DF$attributes)\n  \n  DF.temp <- DF\n  DF.temp <- filter(DF.temp, attributes == \"PD\")\n  \n  \n  # APA appears to use a 6 to 8 digit identifier that needs to be excluded\n  DF.temp$record<-sub(\"[0-9]{6,8}\", \"\", DF.temp$record)\n  \n  #Eliminates \"Bibiliography\", \"Graph\" and \"Table\" from PD field\n  DF.temp$record<-sub(\"[BGT]?[a-z]{4,11}\", \"\", DF.temp$record)\n  \n  # Extract the first portion of the dates, up to the point with a 2 or 4\n  # digit year value. This also captures some letters and characters.\n  DF.temp$record <- stringr::str_extract(DF.temp$record,\n                                         \"[$/A-Za-z0-9]+\\\\d{2,4}\")\n  \n  #Exclude the characters\n  DF.temp$record <- gsub(\"[/A-Za-z]\", \"\", DF.temp$record)\n  \n  #Add 19 to all the records with just two digits\n  DF.temp <- DF.temp[!is.na(DF.temp$record), ]\n  DF.flag <- mutate(DF.temp,\n                    flag = ifelse(nchar(DF.temp$record) == 2, \"1\", \"0\" )) %>%\n    filter(flag == \"1\") %>%\n    group_by(record) %>%\n    summarize(N = n())\n  \n  colnames(DF.flag)<-c(\"Year\", \"N\")\n  \n  DF.temp$record <- ifelse(as.numeric(DF.temp$record) < 100,\n                           paste(\"19\", DF.temp$record, sep=\"\"), DF.temp$record)\n  DF.temp <- mutate(DF.temp, attributes = \"YR\")\n  \n  DF <- rbind(DF, DF.temp)\n  DF <- arrange(DF, articleID)\n  \n  rm(DF.temp)\n  \n  \n  #_______________________________________________________________________________\n  #                            6a. AUTHOR FIELD FIX - DIGITS\n  #-------------------------------------------------------------------------------\n  #\n  # For articles pulled in from SSA, superscripts used to footnote affiliation get\n  # added to author's first name in the AU field\n  #_______________________________________________________________________________\n  \n  DF.temp <- filter(DF, attributes == \"AU\")\n  DF.temp$record <- gsub(\"[[:digit:]]\", \"\", DF.temp$record)\n  DF <- filter(DF, attributes != \"AU\")\n  \n  DF <- rbind(DF, DF.temp)\n  DF <- arrange(DF, articleID)\n  rm(DF.temp)\n  \n  #_______________________________________________________________________________\n  #            6b. AUTHOR FIELD FIX - AUTHORS IN SINGLE FIELD\n  #-------------------------------------------------------------------------------\n  #\n  # Social Work abstracts lists authors in a single cell, separated by a semi-\n  # colon, and then includes digits and email addresses in some occassions.\n  # The following text locates each author in the cell and places it into a new\n  # row to be consistent with PsychInfo and SSA.\n  #_______________________________________________________________________________\n  \n  \n  #Create a new temporary data frame\n  DF.temp <- filter(DF, attributes == \"AU\")\n  \n  #Identify records with semi-colons in author names\n  semi.colons <- grepl(\"(;)\", DF.temp$record)\n  \n  #Select out those records with semi-colons in author names from temporary\n  #data frame\n  DF.temp <- DF.temp[semi.colons, ]\n  \n  #Add a semi colon to the end of every string\n  DF.temp$record <- paste(DF.temp$record, \";\", sep=\"\")\n  semi.colon.split <- strsplit(DF.temp$record, \";\")\n  \n  split.df <- data.frame(\n    attributes = rep(DF.temp$attributes, lapply(semi.colon.split, length)),\n    record = unlist(semi.colon.split),\n    articleID = rep(DF.temp$articleID, lapply(semi.colon.split, length)))\n  \n  #Trim whitespace on both sides\n  split.df$record <- stringr::str_trim(split.df$record, side = \"both\")\n  \n  \n  #The author field is problematic because it contains some email address.\n  #Some fields have been improperly split because they were split on a semi-colon\n  #that was in the middle of the filed.\n  \n  #Create a pattern that eliminates possible emails\n  split.df$record <- ifelse(grepl(\"@\", split.df$record) == TRUE, \"\",\n                            split.df$record)\n  split.df$record <- ifelse(nchar(split.df$record) <= 3, \"\",\n                            split.df$record)\n  split.df$record <- ifelse(grepl(\"\\\\.\", split.df$record) == FALSE, \"\",\n                            split.df$record)\n  split.df$record <- ifelse(grepl(\"&\", split.df$record) == TRUE, \"\",\n                            split.df$record)\n  split.df <- filter(split.df, record != \"\")\n  \n  # Create a vector of all articleID's that were fixed\n  fixed.ID <- unique(split.df$articleID)\n  \n  #Filter out all processed records from the fixed list\n  DF.authors <- filter(DF, attributes == \"AU\")\n  DF.authors.good <- DF.authors[!(DF.authors$articleID %in% fixed.ID),]\n  DF.authors.fixed <- split.df\n  DF.no.authors <- filter(DF, attributes != \"AU\")\n  \n  #Bind the reduced DF with the fixed df\n  DF <- rbind(DF.no.authors, DF.authors.good, DF.authors.fixed)\n  DF <- arrange(DF, articleID)\n  \n  \n  #_______________________________________________________________________________\n  #                       6c. E-mails\n  #-------------------------------------------------------------------------------\n  #\n  # After fixing the author fields in 6b, subsequent testing revealed a large\n  # number of email addresses that were still in the datafile and note excluded.\n  # This section is a patch for this issue.  This code should be re-written\n  # and integrated with the prior section.\n  #_______________________________________________________________________________\n  \n  DF.temp <- filter(DF, attributes == \"AU\")\n  sub.1 <- sub(\"^([^,]*,[^,]*),.*\", \"\\\\1\", DF.temp$record)\n  sub.2 <- sub(\"[,\\\\.][a-zA-Z]{1,}@\", \"\", sub.1)\n  sub.3 <- sub(\"@[a-zA-Z0-9.\\\\]{1,}\", \"\", sub.2)\n  sub.4 <- sub(\"(\\\\s[a-z]{1,})$\", \"\", sub.3)\n  sub.5 <- sub(\"(/&;#]+)\", \"\", sub.4)\n  DF.temp$record <- sub.5\n  \n  DF <- filter(DF, attributes != \"AU\")\n  DF <- rbind(DF, DF.temp)\n  DF <- arrange(DF, articleID)\n  rm(DF.temp)\n  \n  #_______________________________________________________________________________\n  #                       7. Minor Cleaning\n  #-------------------------------------------------------------------------------\n  #\n  # In this section, meaningful variable names are assigned to variables that have\n  # been cleaned and are appropriate for analysis.  All other variables are\n  # excluded to prevent inappropriate analyses.\n  #_______________________________________________________________________________\n  \n  # Exclude UR record from the data file\n  DF$attributes <- ifelse(DF$attributes == \"KW\", \"KP\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"AD\", \"AF\", DF$attributes)\n  \n  DF$attributes <- ifelse(DF$attributes == \"TI\", \"article\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"AU\", \"author\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"SO\", \"journal\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"YR\", \"pubYear\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"AB\", \"abstract\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"KP\", \"keyWord\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"LO\", \"location\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"S2\", \"journalSecondary\",\n                          DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"AF\", \"authorAff\", DF$attributes)\n  DF$attributes <- ifelse(DF$attributes == \"PG\", \"pages\", DF$attributes)\n  \n  variables.to.keep <- c(\"article\", \"author\", \"journal\", \"pubYear\", \"abstract\",\n                         \"keyWord\", \"location\", \"journalSecondary\", \"authorAff\", \"pages\")\n  \n  DF <- DF[DF$attributes %in% variables.to.keep, ]\n  \n  # Strip white-space\n  DF$record <- stringr::str_trim(DF$record, side=\"both\")\n  \n  DF$record <- ifelse(DF$attributes == \"keyWord\", tolower(DF$record), DF$record)\n  \n  \n  # Remove rownames\n  rownames(DF) <- NULL\n  \n  # Reorder variables\n  DF <- select(DF, articleID, attributes, record)\n  \n  rm(blank, path, sub.1, sub.2, sub.3, sub.4, sub.5, variables.to.keep)\n  #_______________________________________________________________________________\n  #                        8. OUTPUT\n  #-------------------------------------------------------------------------------\n  #\n  # This final section places a datafile in the global environment, which is\n  # called bwr.df.  If csv is specified as TRUE in the ebscoBWR function call,\n  # a csv file is written to the user's current working directory.  A few messages\n  # are written to the user's screen, providing a warning message and a few\n  # quality checks to ensure the number of articles matches the number of sources.\n  #_______________________________________________________________________________\n  \n  ebscoBWR.df <<- DF\n  \n  rm(DF)\n  if(csv == TRUE){write.csv(bwr.df, \"ebscoBWR.csv\")}\n  \n  cat(\n    \"****************************************************\n    Wrangling is complete\n    ****************************************************\")\n  \n  if(csv == TRUE){cat(\n    \"\\nThe `ebscoBWR.csv` file can be found in your working directory.\\n\")}\n}\n\n\n",
    "created" : 1441033652242.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2000040059",
    "id" : "F4ACA742",
    "lastKnownWriteTime" : 1441037235,
    "path" : "~/GitHub/BibWrangleR/BibWrangleR/R/ebscoBWR.R",
    "project_path" : "R/ebscoBWR.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}